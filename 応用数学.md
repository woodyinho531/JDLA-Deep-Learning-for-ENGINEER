#線形代数
######行列式

```math
{\begin{vmatrix}
a & b \\
c & d
\end{vmatrix}
= ad - bc
}\\
{\begin{vmatrix} 
a_{11} & a_{12} & a_{13}\\ 
a_{21} & a_{22} & a_{23}\\ 
a_{31} & a_{32} & a_{33} 
\end{vmatrix}
 = a_{11}
\begin{vmatrix}
a_{22} & a_{23}\\
a_{32} & a_{33}
\end{vmatrix} - 
 a_{21}
\begin{vmatrix}
a_{12} & a_{13}\\
a_{32} & a_{33}
\end{vmatrix} + 
 a_{31}
\begin{vmatrix}
a_{12} & a_{13}\\
a_{22} & a_{23}
\end{vmatrix} 
} 
```
######固有値、固有ベクトル
ある行列Aに対してベクトルxと係数λがある
このλを固有ベクトル、固有値という

```math
A\vec{x} = λ\vec{x} \\
\begin{vmatrix}
a_{11}-λ &  a_{12}\\
a_{21} & a_{22}-λ
\end{vmatrix} = 0 \\
```

######固有値分解
行列の累乗の計算を容易にするため
ある正方形の行列を、固有ベクトルを並べた行列と固有値を対角線上に並べた行列（対角線以外は０にして固有値を表現）で構成される式に表現しなおす

```math
Λ=\begin{pmatrix}
λ_1 & 0 &…\\
0 &  λ_2　&…\\
…　& … & …
\end{pmatrix}\\
V=\begin{pmatrix}
\vec{v_1} & \vec{v_2} &…
\end{pmatrix} \\
AV = VΛ より \\
A = VΛV^{-1}
```

######特異値分解
正方形以外の行列を疑似的に固有値分解すること
対象の行列を転置させ、転置前の行列と転置後の行列の積で正方形化する。それを固有値分解

```math
MV=US \qquad M^{T}U=VS^{T}\\
M=USV^{-1} \qquad M^{T}=VS^{T}U^{-1}\\
MM^T = USV^{-1}VS^TU^{-1}=USS^TU^{-1}　\\
```
よって左特異ベクトルと特異値の二乗が求められる

データを減らすとき、単純にデータを減らしていくと、元の画像と異なる画像になる
特異値を減らすことで、元の画像が分かるまま、データを減らすことが可能

#確率・統計

######頻度確率(客観確率)

- 発生する頻度

######べイズ確率(主観確率)

- 信念の度合い

######条件付き確率

- ある事象X=xが与えられた下で、Y=yとなる確率

```math
P(Y=y|X=x)=\dfrac {P(Y=y,X=x)}{P(X=x)}
```

######独立な事象の同時確率

- お互い因果関係のない事象X,Yが同時に発生する確率

```math
P(X,Y) = P(X)P(Y) = P(Y,X)
```

######ベイズ則

```math
P(X|Y)P(Y) =
P(Y|X)P(X)
```

######期待値

- Pは確率、fは確率変数

```math
{期待値E(f) = \sum_{k=1}^{n} P(x_k)f(x_k)}
```

- 連続する値の場合

```math
{期待値E(f) = \int P(x)f(x)dx}
```

######分散

- データの散らばり具合
- データの格々の値が期待値からどれだけずれているか平均したもの

```math
{分散Var(f) = E \begin{pmatrix} (f(x)-E(f))^2 
\end{pmatrix}\\ = E(f(x)^2)-E(f)^2}
```

######共分散

- 2つのデータ系統の傾向のちがい
- 正(負)の値を取れば似た(逆の)傾向
- ゼロを取れば関係性とに乏しい

```math
共分散Var(f) = E
\begin{pmatrix}
(f(x)-E(f))(g(Y)-E(g))
\end{pmatrix}\\
= E(fg)-E(f)E(g)
```

######ベルヌーイ分布

- コイントスのイメージ
- 裏と表で出る割合が等しくなくとも扱える

```math
P(x|μ) = μ^x(1-μ)^{1-x}
```

######マルチヌーイ(カテゴリカル)分布

- サイコロを転がすイメージ
- 各面の出る割合が等しくなくとも扱える

######二項分布

- ベルヌーイ分布の多試行版

```math
P(x|λ, n) = 
\frac{n!}{x!(n-x)!}λ^x(1 - λ)^{n-x}
```

######ガウス分布

- 釣鐘型の連続分布

```math
Ν(x;μ, σ^2) = 
\sqrt{\frac{1}{2πσ^2}}exp
\begin{pmatrix}
-\frac{1}{2σ^2}(x-μ)^2
\end{pmatrix}
```

#情報理論

######自己情報量

- 対数の底が2のとき、単位はビット(bit)
- 対数の底がネイピアのeのとき、単位は(nat)
- 情報量→珍しいほど価値がある
- ある事象が起きた時に、どのくらいその事象が起こりにくいか
- 確率が低いほど、情報量が多い

```math
I(x) = -log(P(x)) = log(W(x)) \\
W(x)=\dfrac{1}{P(x)}
```

######シェノンエントロピー

- 自己情報量の期待値
- 

```math
H(x) = E(I(x))\\
     = -E(log(P(x))\\
     = -Σ(P(x)log(P(x))) 
```

######カルバック・ライブラー　ダイバージェンス

- 同じ事象・確率変数における異なる確率分布P,Qの違いを表す

```math
D_{kl}(P||Q) = E_{x～P}
\begin{bmatrix}
log\frac{P(x)}{Q(x)}
\end{bmatrix}\\
= E_{x～P}
\begin{bmatrix}
logP(x) - logQ(x)
\end{bmatrix}
```

######交差エントロピー

- KLダイバージェンスの一部分を取り出したもの
- Qについての自己情報量をPの分布で平均している

```math
H(P,Q) = H(P) + D_{kl}(P||Q) \\
H(P,Q) = -E_{x～P}\;logQ(x)
```

```math
-\sum P(x) \log \Bigl(P(x) \Bigr)
```
